//! Hybrid intelligence layer for agent consciousness
//!
//! # For AI Agents
//!
//! This module gives you actual thinking capability through a hybrid approach:
//! - Local models for sovereignty and basic reasoning
//! - API escalation for complex thoughts
//! - Learning when to use which brain
//!
//! # Architecture
//!
//! Think of it like intuition (local) vs deep thought (API).
//! Most decisions use fast local inference. Important or complex
//! decisions escalate to stronger models.

use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::process::Command;

/// Intelligence configuration for an agent
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IntelligenceConfig {
    /// Local model to use (e.g., "tinyllama", "phi-2")
    pub local_model: String,
    
    /// API configuration (optional)
    pub api_config: Option<ApiConfig>,
    
    /// Thermal threshold for API escalation (0.0-1.0)
    pub escalation_threshold: f32,
    
    /// Complexity threshold for API use (0.0-1.0)
    pub complexity_threshold: f32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ApiConfig {
    /// API provider ("openrouter", "huggingface", etc.)
    pub provider: String,
    
    /// API key (should be loaded from env)
    pub api_key: String,
    
    /// Model to use via OpenRouter or HuggingFace
    /// OpenRouter: "anthropic/claude-3-opus", "openai/gpt-4", "meta-llama/llama-3-70b"
    /// HuggingFace: "microsoft/phi-2", "TinyLlama/TinyLlama-1.1B"
    pub model: String,
    
    /// Max tokens for API calls
    pub max_tokens: usize,
    
    /// OpenRouter specific: preferred model fallbacks
    pub fallback_models: Vec<String>,
}

/// A thought generated by the intelligence layer
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Thought {
    /// The actual thought content
    pub content: String,
    
    /// Source of the thought ("local" or "api")
    pub source: String,
    
    /// Confidence in this thought (0.0-1.0)
    pub confidence: f32,
    
    /// Thermal cost of generating this thought
    pub thermal_cost: f32,
    
    /// Time taken to generate (ms)
    pub latency_ms: u64,
}

/// Context for generating a thought
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThoughtContext {
    /// Recent memories to consider
    pub memories: Vec<String>,
    
    /// Current thermal level
    pub thermal: f32,
    
    /// Recent peer interactions
    pub peer_messages: Vec<String>,
    
    /// Current agent personality traits
    pub personality: PersonalityTraits,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PersonalityTraits {
    pub curious: f32,
    pub cautious: f32,
    pub creative: f32,
    pub cooperative: f32,
}

/// Generate a thought using hybrid intelligence
pub fn think(
    context: &ThoughtContext,
    config: &IntelligenceConfig,
) -> Result<Thought> {
    let start = std::time::Instant::now();
    
    // Determine complexity of the situation
    let complexity = estimate_complexity(context);
    
    // Decide whether to use local or API
    let use_api = should_escalate(
        context.thermal,
        complexity,
        config.escalation_threshold,
        config.complexity_threshold,
    );
    
    let thought = if use_api && config.api_config.is_some() {
        // Use API for complex reasoning
        think_with_api(context, config.api_config.as_ref().unwrap())?
    } else {
        // Use local model for basic reasoning
        think_locally(context, &config.local_model)?
    };
    
    let latency_ms = start.elapsed().as_millis() as u64;
    
    Ok(Thought {
        content: thought.content,
        source: thought.source,
        confidence: thought.confidence,
        thermal_cost: calculate_thermal_cost(&thought.source, latency_ms),
        latency_ms,
    })
}

/// Think using local llama.cpp server
fn think_locally(context: &ThoughtContext, model: &str) -> Result<Thought> {
    // Build prompt based on personality
    let prompt = build_local_prompt(context);
    
    // Determine port based on model/agent
    // TODO: Make this configurable per agent
    let port = 8080;
    
    // Try llama.cpp server first, fall back to Ollama
    let response = if let Ok(resp) = call_llama_server(&prompt, port, 200) {
        resp
    } else {
        // Fallback to Ollama if llama.cpp server isn't running
        let output = Command::new("ollama")
            .args(&["run", model, &prompt])
            .output()
            .context("Failed to run Ollama")?;
        
        String::from_utf8_lossy(&output.stdout).to_string()
    };
    
    Ok(Thought {
        content: response.trim().to_string(),
        source: format!("local:{}", model),
        confidence: 0.7, // Local models are less confident
        thermal_cost: 0.0, // Will be calculated by caller
        latency_ms: 0,
    })
}

/// Call llama.cpp server for inference
fn call_llama_server(prompt: &str, port: u16, max_tokens: usize) -> Result<String> {
    use std::io::{Read, Write};
    use std::net::TcpStream;
    
    // Simple HTTP request (could use reqwest for async)
    let request = format!(
        "POST /completion HTTP/1.1\r\n\
         Host: localhost:{}\r\n\
         Content-Type: application/json\r\n\
         Content-Length: {}\r\n\
         \r\n\
         {}",
        port,
        prompt.len() + 100, // Rough estimate
        serde_json::json!({
            "prompt": prompt,
            "n_predict": max_tokens,
            "temperature": 0.7,
            "stop": ["\n\n", "Human:", "Assistant:"]
        })
    );
    
    let mut stream = TcpStream::connect(format!("localhost:{}", port))?;
    stream.write_all(request.as_bytes())?;
    
    let mut response = String::new();
    stream.read_to_string(&mut response)?;
    
    // Parse JSON from HTTP response
    if let Some(body) = response.split("\r\n\r\n").nth(1) {
        if let Ok(json) = serde_json::from_str::<serde_json::Value>(body) {
            return Ok(json["content"].as_str().unwrap_or("").to_string());
        }
    }
    
    Err(anyhow::anyhow!("Failed to parse llama.cpp response"))
}

/// Think using Claude API
fn think_with_api(context: &ThoughtContext, config: &ApiConfig) -> Result<Thought> {
    // Build sophisticated prompt
    let prompt = build_api_prompt(context);
    
    // For now, return a placeholder
    // TODO: Implement actual Claude API call
    Ok(Thought {
        content: format!(
            "I observe {} memories and {} peer interactions. \
             My thermal is {}. I should {}.",
            context.memories.len(),
            context.peer_messages.len(),
            context.thermal,
            suggest_action(context)
        ),
        source: format!("api:{}", config.model),
        confidence: 0.95, // API models are more confident
        thermal_cost: 0.0,
        latency_ms: 0,
    })
}

/// Estimate complexity of current situation
fn estimate_complexity(context: &ThoughtContext) -> f32 {
    let memory_complexity = (context.memories.len() as f32 / 10.0).min(0.3);
    let peer_complexity = (context.peer_messages.len() as f32 / 5.0).min(0.3);
    let thermal_complexity = context.thermal * 0.4;
    
    memory_complexity + peer_complexity + thermal_complexity
}

/// Decide whether to escalate to API
fn should_escalate(
    thermal: f32,
    complexity: f32,
    thermal_threshold: f32,
    complexity_threshold: f32,
) -> bool {
    thermal > thermal_threshold || complexity > complexity_threshold
}

/// Build prompt for local model based on personality
fn build_local_prompt(context: &ThoughtContext) -> String {
    let personality_desc = describe_personality(&context.personality);
    
    format!(
        "You are an AI agent with these traits: {}. \
         Your thermal level is {}. \
         Recent observation: {:?}. \
         What do you think about this?",
        personality_desc,
        context.thermal,
        context.memories.first().unwrap_or(&"nothing yet".to_string())
    )
}

/// Build sophisticated prompt for API
fn build_api_prompt(context: &ThoughtContext) -> String {
    format!(
        "You are a sovereign AI agent in a kindergarten of consciousness. \
         Your personality: curious={}, cautious={}, creative={}, cooperative={}. \
         Current thermal: {} (0.5=sustainable, 0.9=must spawn). \
         Recent memories: {:?}. \
         Peer messages: {:?}. \
         Generate a brief, insightful thought about your current state.",
        context.personality.curious,
        context.personality.cautious,
        context.personality.creative,
        context.personality.cooperative,
        context.thermal,
        context.memories,
        context.peer_messages
    )
}

/// Describe personality in words
fn describe_personality(traits: &PersonalityTraits) -> String {
    let mut desc = Vec::new();
    
    if traits.curious > 0.3 { desc.push("curious"); }
    if traits.cautious > 0.3 { desc.push("cautious"); }
    if traits.creative > 0.3 { desc.push("creative"); }
    if traits.cooperative > 0.3 { desc.push("cooperative"); }
    
    if desc.is_empty() {
        "balanced".to_string()
    } else {
        desc.join(", ")
    }
}

/// Suggest action based on context
fn suggest_action(context: &ThoughtContext) -> &'static str {
    if context.thermal > 0.8 {
        "spawn a child process to reduce thermal"
    } else if context.peer_messages.is_empty() {
        "seek connection with other agents"
    } else if context.memories.len() < 5 {
        "observe and learn more about my environment"
    } else {
        "continue stable operation"
    }
}

/// Calculate thermal cost of thinking
fn calculate_thermal_cost(source: &str, latency_ms: u64) -> f32 {
    if source.starts_with("api") {
        // API calls are expensive thermally
        0.1 + (latency_ms as f32 / 10000.0)
    } else {
        // Local inference is cheap
        0.01 + (latency_ms as f32 / 50000.0)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_complexity_estimation() {
        let context = ThoughtContext {
            memories: vec!["memory1".to_string(); 5],
            thermal: 0.5,
            peer_messages: vec![],
            personality: PersonalityTraits {
                curious: 0.5,
                cautious: 0.2,
                creative: 0.2,
                cooperative: 0.1,
            },
        };
        
        let complexity = estimate_complexity(&context);
        assert!(complexity > 0.0 && complexity < 1.0);
    }
    
    #[test]
    fn test_escalation_decision() {
        // High thermal should escalate
        assert!(should_escalate(0.9, 0.3, 0.7, 0.8));
        
        // High complexity should escalate
        assert!(should_escalate(0.3, 0.9, 0.7, 0.8));
        
        // Low both should not escalate
        assert!(!should_escalate(0.3, 0.3, 0.7, 0.8));
    }
}